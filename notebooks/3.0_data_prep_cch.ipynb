{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI TEXT GENERATION FOR BEGINNERS: CREATING AND FINE TUNING YOUR OWN SINGLISH CHATBOT\n",
    "\n",
    "Text generation is one of the most exciting areas in modern NLP, thanks to the work by companies like Hugging Face and OpenAI. It is also one of the harder areas for NLP beginners to navigate, due to the higher technical and resource barrier to entry (access to GPUs for training, for instance). Finding an appropriate dataset for your own experiment can be just as tough. \n",
    "\n",
    "For this third installment of notebooks on practical NLP tasks, I'll be sharing a series of notebooks that I've created and adapted from other sources for my own learning. The goal is to speed up your own learning progress, so that more time is spent on understanding the data and experimentation, instead of finding the basic building blocks for the project.\n",
    "\n",
    "The final product is far from polished, and reflects the limits of my knowledge and the suitability of the dataset. But as they say, don't let the perfect get in the way of the good. Things are moving very fast in this area, and it would be a shame to sit it out while waiting for the perfect conditions.\n",
    "\n",
    "This series contains a combination of Colab and local-machine-based notebooks. I would recommend upgrading to Colab Pro and increasing your Google account storage limits if you can afford it. You can still run a limited version of the experiments here if you don't wish to upgrade. But bear in mind you'll have to manage the storage limits quite regularly as the fine tuning process tend to generate pretty huge files. 15Gb of free storage sounds like a lot, but it actually isn't once you get started....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: DATA EXTRACTION AND PREPARATION\n",
    "\n",
    "Finding the right dataset for your use case is likely the biggest stumbling block to getting started. While online tutorials tend to come with demo datasets, they may not appeal to you or the audience you are building for.\n",
    "\n",
    "For this project, I'm focusing on [Singlish](https://en.wikipedia.org/wiki/Singlish), or colloquial Singaporean English. It's a mish-mash of several languages and local slang, and can be bewildering for native English speakers who have never been to Singapore.\n",
    "\n",
    "The first half of the data preparation is unique to the corpus used - a collection of [SMS messages by Singaporean students at a local university](https://scholarbank.nus.edu.sg/handle/10635/137343).\n",
    "\n",
    "The second half of the data preparation is unique to the requirements of the chatbot finetuning, as outlined in this [Colab notebook](https://colab.research.google.com/drive/15wa925dj7jvdvrz8_z3vU7btqAFQLVlG) which I've based most of the finetuning code on. \n",
    "\n",
    "If you use a different dataset, you'll have to change the first half of the processing accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. EXTRACTING DATA FROM THE JSON FILE\n",
    "\n",
    "The SMSes are nested pretty deeply in the original json file. Next couple of cells are aimed at extracting the data into a dataframe format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = [json.loads(line) for line in open('../data/singlish.json', 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smsCorpus.@date</th>\n",
       "      <th>smsCorpus.@version</th>\n",
       "      <th>smsCorpus.message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015.03.09</td>\n",
       "      <td>1.2</td>\n",
       "      <td>[{'@id': 10120, 'text': {'$': 'Bugis oso near ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  smsCorpus.@date  smsCorpus.@version  \\\n",
       "0      2015.03.09                 1.2   \n",
       "\n",
       "                                   smsCorpus.message  \n",
       "0  [{'@id': 10120, 'text': {'$': 'Bugis oso near ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.json_normalize(raw)\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_messages = pd.concat(\n",
    "    df_raw[\"smsCorpus.message\"]\n",
    "    .apply(pd.DataFrame)\n",
    "    .tolist(),\n",
    "    keys=df_raw[\"smsCorpus.@date\"],\n",
    "    sort=False,\n",
    ").reset_index(level=\"smsCorpus.@date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_messages['sms_text'] = [x.get('$') for x in raw_messages['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = pd.json_normalize(raw_messages['source'], meta='@id')\n",
    "\n",
    "destination = pd.json_normalize(raw_messages['destination'], meta='@id')\n",
    "\n",
    "profile = pd.json_normalize(raw_messages['messageProfile'], meta='@id')\n",
    "\n",
    "collection = pd.json_normalize(raw_messages['collectionMethod'], meta='@id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_raw = pd.concat([raw_messages, source, destination, profile, collection], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"@id\",\n",
    "    \"userProfile.userID.$\",\n",
    "    \"sms_text\",\n",
    "    \"userProfile.country.$\",\n",
    "    \"userProfile.age.$\",\n",
    "    \"userProfile.gender.$\",\n",
    "    \"srcNumber.$\",\n",
    "    \"phoneModel.@manufactuer\",\n",
    "    \"phoneModel.@smartphone\",\n",
    "    \"userProfile.frequency.$\",\n",
    "]\n",
    "\n",
    "sms = sms_raw[cols].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['sms_text'] = sms['sms_text'].astype('str')\n",
    "\n",
    "# simple function to clean the text and remove non-ascii characters\n",
    "def clean_text(text):    \n",
    "    text = text.encode(\"ascii\", errors=\"ignore\").decode(\"ascii\") #remove non-ascii, Chinese characters\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    text = re.sub(r\"\\n\\n\", \" \", text)\n",
    "    text = re.sub(r\"\\W\", \" \", text)\n",
    "    text = re.sub(r\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)\n",
    "    text = text.strip(\" \")\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(' +',' ', text).strip() # get rid of multiple spaces and replace with a single    \n",
    "    return text\n",
    "\n",
    "sms[\"clean_text\"] = sms['sms_text'].map(lambda text: clean_text(text))\n",
    "\n",
    "sms = sms.dropna(subset=['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding a word count col for filtering\n",
    "\n",
    "sms['word_count'] = sms['clean_text'].str.count(' ') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# narrowing down col selection\n",
    "\n",
    "cols = [\"@id\", \"userProfile.userID.$\", \"userProfile.country.$\", \"sms_text\", \"clean_text\", \"word_count\"]\n",
    "\n",
    "sms = sms[cols].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming cols for clarity\n",
    "\n",
    "sms = sms.rename(\n",
    "    columns={\n",
    "        \"@id\": \"data_id\",\n",
    "        \"userProfile.userID.$\": \"user_id\",\n",
    "        \"userProfile.country.$\": \"country\",\n",
    "        \"sms_text\": \"sms_text\",\n",
    "        \"clean_text\": \"clean_text\",\n",
    "        \"word_count\": \"word_count\",\n",
    "\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55835, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>country</th>\n",
       "      <th>sms_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10120</td>\n",
       "      <td>51</td>\n",
       "      <td>SG</td>\n",
       "      <td>Bugis oso near wat...</td>\n",
       "      <td>Bugis oso near wat</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10121</td>\n",
       "      <td>51</td>\n",
       "      <td>SG</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>Go until jurong point crazy Available only in ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10122</td>\n",
       "      <td>51</td>\n",
       "      <td>SG</td>\n",
       "      <td>I dunno until when... Lets go learn pilates...</td>\n",
       "      <td>I dunno until when Lets go learn pilates</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10123</td>\n",
       "      <td>51</td>\n",
       "      <td>SG</td>\n",
       "      <td>Den only weekdays got special price... Haiz......</td>\n",
       "      <td>Den only weekdays got special price Haiz Cant ...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10124</td>\n",
       "      <td>51</td>\n",
       "      <td>SG</td>\n",
       "      <td>Meet after lunch la...</td>\n",
       "      <td>Meet after lunch la</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_id user_id country                                           sms_text  \\\n",
       "0    10120      51      SG                              Bugis oso near wat...   \n",
       "1    10121      51      SG  Go until jurong point, crazy.. Available only ...   \n",
       "2    10122      51      SG     I dunno until when... Lets go learn pilates...   \n",
       "3    10123      51      SG  Den only weekdays got special price... Haiz......   \n",
       "4    10124      51      SG                             Meet after lunch la...   \n",
       "\n",
       "                                          clean_text  word_count  \n",
       "0                                 Bugis oso near wat           4  \n",
       "1  Go until jurong point crazy Available only in ...          20  \n",
       "2           I dunno until when Lets go learn pilates           8  \n",
       "3  Den only weekdays got special price Haiz Cant ...          25  \n",
       "4                                Meet after lunch la           4  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 CUTTING OUT THE NOISE\n",
    "\n",
    "A bigger dataset isn't necessarily a better one if it is merely noisy. Prior to creating the training and validation sets, I filtered out SMSes of 3 words or less (too few words) and kept only those sent by users in Singapore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit1 = sms['word_count'] > 3\n",
    "crit2 = sms['country'] == 'SG'\n",
    "crit3 = sms['country'] == 'Singapore'\n",
    "\n",
    "sms = sms[crit1 & (crit2 | crit3)].copy().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CREATE TRAIN-VALIDATION SETS\n",
    "\n",
    "The data format for training a chatbot is different from the usual CSV files I've encountered. But it sort of makes sense in that for every response, the model will be fed x-number of previous SMSes as \"context\".\n",
    "\n",
    "In this case, I'm using 7 previous responses for context. You can increase or decrease the number as you wish. Here's the [link to another Colab file](https://colab.research.google.com/drive/1kKErlSSpewQbWexFPEj1rPWsYpMx69ZS?usp=sharing) that shows you how to tweak your dataset ahead of fine-tuning the DialoGPT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexted = []\n",
    "\n",
    "n = 7\n",
    "\n",
    "for i in range(n, len(sms['clean_text'])):\n",
    "    row = []\n",
    "    prev = i - 1 - n # we additionally substract 1, so row will contain current response and 7 previous responses  \n",
    "    for j in range(i, prev, -1):\n",
    "        row.append(sms['clean_text'][j])\n",
    "    contexted.append(row)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['response', 'context'] \n",
    "columns = columns + ['context/'+str(i) for i in range(n-1)]\n",
    "\n",
    "df = pd.DataFrame.from_records(contexted, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29353, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "      <th>context/2</th>\n",
       "      <th>context/3</th>\n",
       "      <th>context/4</th>\n",
       "      <th>context/5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey pple or for nights Excellent location wif ...</td>\n",
       "      <td>nights We nt staying at port step liao Too ex</td>\n",
       "      <td>m walking in citylink now faster come down Me ...</td>\n",
       "      <td>Meet after lunch la</td>\n",
       "      <td>Den only weekdays got special price Haiz Cant ...</td>\n",
       "      <td>I dunno until when Lets go learn pilates</td>\n",
       "      <td>Go until jurong point crazy Available only in ...</td>\n",
       "      <td>Bugis oso near wat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yun ah the ubi one say if wan call by tomorrow...</td>\n",
       "      <td>Hey pple or for nights Excellent location wif ...</td>\n",
       "      <td>nights We nt staying at port step liao Too ex</td>\n",
       "      <td>m walking in citylink now faster come down Me ...</td>\n",
       "      <td>Meet after lunch la</td>\n",
       "      <td>Den only weekdays got special price Haiz Cant ...</td>\n",
       "      <td>I dunno until when Lets go learn pilates</td>\n",
       "      <td>Go until jurong point crazy Available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey tmr maybe can meet you at yck</td>\n",
       "      <td>Yun ah the ubi one say if wan call by tomorrow...</td>\n",
       "      <td>Hey pple or for nights Excellent location wif ...</td>\n",
       "      <td>nights We nt staying at port step liao Too ex</td>\n",
       "      <td>m walking in citylink now faster come down Me ...</td>\n",
       "      <td>Meet after lunch la</td>\n",
       "      <td>Den only weekdays got special price Haiz Cant ...</td>\n",
       "      <td>I dunno until when Lets go learn pilates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oh i asked for fun Haha take care</td>\n",
       "      <td>Hey tmr maybe can meet you at yck</td>\n",
       "      <td>Yun ah the ubi one say if wan call by tomorrow...</td>\n",
       "      <td>Hey pple or for nights Excellent location wif ...</td>\n",
       "      <td>nights We nt staying at port step liao Too ex</td>\n",
       "      <td>m walking in citylink now faster come down Me ...</td>\n",
       "      <td>Meet after lunch la</td>\n",
       "      <td>Den only weekdays got special price Haiz Cant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We are supposed to meet to discuss abt our tri...</td>\n",
       "      <td>Oh i asked for fun Haha take care</td>\n",
       "      <td>Hey tmr maybe can meet you at yck</td>\n",
       "      <td>Yun ah the ubi one say if wan call by tomorrow...</td>\n",
       "      <td>Hey pple or for nights Excellent location wif ...</td>\n",
       "      <td>nights We nt staying at port step liao Too ex</td>\n",
       "      <td>m walking in citylink now faster come down Me ...</td>\n",
       "      <td>Meet after lunch la</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            response  \\\n",
       "0  Hey pple or for nights Excellent location wif ...   \n",
       "1  Yun ah the ubi one say if wan call by tomorrow...   \n",
       "2                  Hey tmr maybe can meet you at yck   \n",
       "3                  Oh i asked for fun Haha take care   \n",
       "4  We are supposed to meet to discuss abt our tri...   \n",
       "\n",
       "                                             context  \\\n",
       "0      nights We nt staying at port step liao Too ex   \n",
       "1  Hey pple or for nights Excellent location wif ...   \n",
       "2  Yun ah the ubi one say if wan call by tomorrow...   \n",
       "3                  Hey tmr maybe can meet you at yck   \n",
       "4                  Oh i asked for fun Haha take care   \n",
       "\n",
       "                                           context/0  \\\n",
       "0  m walking in citylink now faster come down Me ...   \n",
       "1      nights We nt staying at port step liao Too ex   \n",
       "2  Hey pple or for nights Excellent location wif ...   \n",
       "3  Yun ah the ubi one say if wan call by tomorrow...   \n",
       "4                  Hey tmr maybe can meet you at yck   \n",
       "\n",
       "                                           context/1  \\\n",
       "0                                Meet after lunch la   \n",
       "1  m walking in citylink now faster come down Me ...   \n",
       "2      nights We nt staying at port step liao Too ex   \n",
       "3  Hey pple or for nights Excellent location wif ...   \n",
       "4  Yun ah the ubi one say if wan call by tomorrow...   \n",
       "\n",
       "                                           context/2  \\\n",
       "0  Den only weekdays got special price Haiz Cant ...   \n",
       "1                                Meet after lunch la   \n",
       "2  m walking in citylink now faster come down Me ...   \n",
       "3      nights We nt staying at port step liao Too ex   \n",
       "4  Hey pple or for nights Excellent location wif ...   \n",
       "\n",
       "                                           context/3  \\\n",
       "0           I dunno until when Lets go learn pilates   \n",
       "1  Den only weekdays got special price Haiz Cant ...   \n",
       "2                                Meet after lunch la   \n",
       "3  m walking in citylink now faster come down Me ...   \n",
       "4      nights We nt staying at port step liao Too ex   \n",
       "\n",
       "                                           context/4  \\\n",
       "0  Go until jurong point crazy Available only in ...   \n",
       "1           I dunno until when Lets go learn pilates   \n",
       "2  Den only weekdays got special price Haiz Cant ...   \n",
       "3                                Meet after lunch la   \n",
       "4  m walking in citylink now faster come down Me ...   \n",
       "\n",
       "                                           context/5  \n",
       "0                                 Bugis oso near wat  \n",
       "1  Go until jurong point crazy Available only in ...  \n",
       "2           I dunno until when Lets go learn pilates  \n",
       "3  Den only weekdays got special price Haiz Cant ...  \n",
       "4                                Meet after lunch la  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the df into training andd validation set\n",
    "\n",
    "train_df, validate_df = train_test_split(df, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23482, 8), (5871, 8))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, validate_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the 2 lines below to generate the CSV files for training in the next notebook\n",
    "\n",
    "#train_df.to_csv('../data/train_df.csv', index=False)\n",
    "#validate_df.to_csv('../data/validate_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. OPTIONAL: ADDITIONAL DATASET FOR AITEXTGEN\n",
    "\n",
    "There are a number of other options out there for those who want to experiment further with text generation. One interesting library is [aitextgen](https://github.com/minimaxir/aitextgen). I didn't get very good results from the dataset, but I'm including the option here for those who want to try it out in any case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sms_text = sms['clean_text'].values.tolist()\n",
    "\n",
    "#with open(\"../data/singlish_sms.txt\", \"w\") as output:\n",
    "#    output.write(str(sms_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
